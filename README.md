## S√©ries Temporais
#### Objetivos:
- Analisar a origem da s√©rie
- Previs√µes futuras
- Descri√ß√£o do comportamento da s√©rie 
- Analisar perodicidade ou tend√™ncia 

#### Tipos:
- Univariada = apenas uma vari√°vel se altera ao longo do tempo
- Multivariada = mais de uma vari√°vel se altera ao longo do tempo

#### Conceitos:
S√©rie Temporal -> √© um conjunto de observa√ß√µes ordenadas no tempo ou um corte particular de um processo estoc√°stico desconhecido

#### Matematicamente: Y = Tdt + Szt + et 
- Tend√™ncia (Tdt): Mudan√ßas graduais em longo prazo (crescimento populacional).
- Sazonalidade (Szt): oscila√ß√µes de subida e de queda que sempre ocorrem em um determinado per√≠odo (maior valor da conta de energia el√©trica no inverno).
- Res√≠duos (et): apresenta movimentos ascendentes e descendentes da s√©rie ap√≥s a retirada do efeito de tend√™ncia ou sazonal (sequ√™ncia de vari√°veis aleat√≥rias).

![Figura-4-Decomposicao-da-serie-temporal-em-componentes-de-sazonalidade-de-tendencia-e](https://github.com/HenrySchall/Time-Series/assets/96027335/46bf2b49-dcb1-4153-9d66-2e57b4dc57ad)

Processo Estoc√°stico -> √© uma cole√ß√£o de vari√°veis aleat√≥rias definidas num mesmo espa√ßo de probabilidades (processo gerador de uma s√©rie de vari√°veis). A descri√ß√£o de um 
processo estoc√°stico √© feita atrav√©s de uma distribui√ß√£o de probabilidade conjunta (o que √© muito complexo de se fazer), ent√£o geralmente descrevemos ele por meio das fun√ß√µes:
- $ùúá(ùë°)=ùê∏{ùëç(ùë°)}$ -> M√©dia 
- $ùúé^2(ùë°)=ùëâùëéùëü{ùëç(ùë°)}$ -> Vari√¢ncia 
- $ùõæ(ùë°1,ùë°2)=ùê∂ùëúùë£{ùëç(ùë°1),ùëç(ùë°2)}$ -> Autocovari√¢ncia

![Captura de tela 2024-07-04 180109](https://github.com/HenrySchall/Time-Series/assets/96027335/7ffc0399-4f35-4e82-ac69-8950c083c8f4)

Estacionaridade -> √© quando uma s√©rie temporal apresenta todas suas caracter√≠sticas estat√≠sticas constante ao longo do tempo
- Estacionaridade Fraca = √© quando as propriedades estatiaticas, s√£o constantes no tempo, E(x)=U, Var(x) = ùúé^2, COV(X,X-n) = k (corari√¢ncia entre observa√ß√µes em diferentes pontos no tempo depende do tempo espec√≠fico em que elas ocorreram). Na literatura, geralmente estacionalidade significa estacionalidade fraca.

- Estacionaridade Forte = tamb√©m chamada de estrita, √© quando a fun√ß√£o de probabilidade conjunta √© invariante no tempo, ou seja, as distribui√ß√µes individuais s√£o iguais para todos "ts". Com isso a covari√¢ncia depende apenas da dist√¢ncia entre as observa√ß√µes e n√£o do tempo especifico que ocorreram. 

![Imagem-2](https://github.com/HenrySchall/Time-Series/assets/96027335/6c237676-00e5-407f-bcc7-cddf6c1c4a34)

Passeio Aleat√≥rio (Random Walk) -> √© a soma de pequenas flutua√ß√µes estoc√°sticas (tend√™ncia estoc√°stica)
Matematicamente: $ùëçùë° = ùëç(ùë°‚àí1)+ et$

Autocorrela√ß√£o -> √© a correla√ß√£o de determinados per√≠odos anteriores com o per√≠odo atual, ou seja, o grau de depend√™ncia serial. Cada per√≠odo desse tipo de correla√ß√£o √© denominado lag (defasagem) e sua representa√ß√£o √© feita pela Fun√ß√£o de Autocorrela√ß√£o (FAC) e a Fun√ß√£o de Autocorrela√ß√£o Parcial (FACP), ambas comparam o valor presente com os valores passados da s√©rie, a diferen√ßa entre eles √© que a FAC analisa tanto a correla√ß√£o direta como a indireta, j√° a FACP apenas correla√ß√£o direta. Ent√£o podemos dizer, que a FAC v√™ a correla√ß√£o direta do m√™s de janeiro em mar√ßo e tamb√©m a correla√ß√£o indireta que o m√™s de janeiro teve em fevereiro que tamb√©m teve em mar√ßo, enquanto que a FACP apenas a correla√ß√£o de janeiro em mar√ßo. Essa an√°lise √© feita, porque √© o pressuposto essencial para se criar previs√µes eficientes de uma s√©rie.

Ru√≠do Branco (White Noise) -> √© quando o erro de uma s√©rie temporal, segue uma distribui√ß√£o normal, ou seja, um processo puramente aleat√≥rio. 
- $E(Xt) = 0$ 
- $Var(Xt) = ùúé^2$

Transforma√ß√£o e Suaviza√ß√£o -> S√£o t√©cnicas que buscam deixar a s√©rie o mais pr√≥ximo poss√≠vel de uma distribui√ß√£o normal. Transformando o valor das var√°veis ou suavizando a tend√™ncia e/ou sazonaliade da s√©rie. Dentre todas as t√©cnicas existentes podemos citar:
1) Tranforma√ß√£o Log 
2) Tranforma√ß√£o Expoencial
3) Tranforma√ß√£o Box-Cox
4) Suaviza√ß√£o M√©dia M√≥vel Exponencial (MME) - Curto per√≠odo 
5) Suaviza√ß√£o por M√©dia M√≥vel Simples (MMS) - Longo per√≠odo

Diferencia√ß√£o -> A diferencia√ß√£o, busca transformar uma s√©rie n√£o estacion√°ria em estacion√°ria, por meio da diferen√ßa de dois per√≠odos consecutivos

#### Modelos das s√©ries temporais univariados:
Modelos lineares:
 - Modelos autorregressivos (AR)
 - Modelos m√©dias m√≥veis (MA)
 - Modelos autorregressivos e m√©dias m√≥veis (ARMA)
 - Modelos autorregressivos integrados e de m√©dias m√≥veis (ARIMA)
 - Modelos de longas depend√™ncias temporais ou mem√≥ria longa (ARFIMA)
 - Modelos autorregressivos integrados e de m√©dias m√≥veis com sazonalidade (SARIMA)
 
Modelos n√£o lineares:
 - Autorregressivo com limiar (TAR)
 - Autorregressivo com transi√ß√£o suave (STAR)
 - Troca de regime markoviano (MSM)
 - Redes neurais artificiais autorregressivas (AR-ANN)

Estrutura: 
- Autorregressivo (AR): indica que a vari√°vel √© regressada¬†em seus valores anteriores. 
- Integrado (I): indica que os valores de dados foram substitu√≠dos com a diferen√ßa entre seus valores e os valores anteriores (diferencia√ß√£o).
- M√©dia m√≥vel (MA): Indica que o erro de regress√£o √© uma combina√ß√£o linear¬†dos termos de erro dos valores passados.

Codifica√ß√£o: (p, d, q) Par√¢metro d s√≥ pode ser inteiro, caso estivessemos trabalhando com um Modelo ARFIMA, o par√¢metro d pode ser fracionado

- p = ordem da autorregress√£o.
- d = grau de diferencia√ß√£o.
- q = ordem da m√©dia m√≥vel.

Quando adicionamos a sazonalidade, al√©m da codifica√ß√£o Arima (p, d, q), incluimos a codifica√ß√£o para a Sazonalidade (P, D, Q). Ent√£o um modelo SARIMA √© definido por: (p, d, q)(P, D, Q)

Exemplos:
- Modelo ARFIMA: (1, 0.25, 1) 
- Modelo ARIMA: (2, 1, 1)
- Modelo AR: (1, 0, 0)
- Modelo MA (0, 0, 3)
- Modelo I: (0, 2, 0)
- Modelo ARMA: (4, 0, 1)
- Modelo SARIMA: (1, 1, 2)(2, 0, 1)
- 
#### Fun√ß√£o de Autocorrela√ß√£o (FAC) e Fun√ß√£o de Autocorrela√ß√£o Parcial (FACP)

#### Akaike‚Äôs Information Criterion (AIC) e o Bayesian Information Criterion (BIC)
Nos modelos mais avan√ßados, as fun√ß√µes de autocorrela√ß√£o e autocorrela√ß√£o parcial n√£o s√£o informativas para definir a ordem dos modelos, por isso usasse um crit√©rio de informa√ß√£o. Um crit√©rio de informa√ß√£o √© uma forma de encontrar o n√∫mero ideal de par√¢metros de um modelo, para entend√™-lo, tenha em mente que, a cada regressor adicional, a soma dos res√≠duos n√£o vai aumentar; frequentemente, diminuir√°. A redu√ß√£o se d√° √† custa de mais regressores. Para balancear a redu√ß√£o dos erros e o aumento do n√∫mero de regressores, o crit√©rio de informa√ß√£o associa uma penalidade a esse aumento. Sendo assim, sua equa√ß√£o apresenta duas partes: a primeira mede a qualidade do ajuste do modelo aos dados, enquanto a segunda parte √© chamada de fun√ß√£o de penaliza√ß√£o dado que penaliza modelos com muitos par√¢metros, sendo assim, dado todas as combina√ß√µes de modelos procuramos aquele que apresenta menor AIC.

#### Testes Estat√≠sticos

##### Teste de Kolmogorov-Smirnov
> Qualifica a m√°xima diferen√ßa absoluta entre a fun√ß√£o de distribui√ß√£o da amostra e a fun√ß√£o de distribui√ß√£o acumulada da distribui√ß√£o de refer√™ncia (geramente distribui√ß√£o normal), ou seja, ele qualifica dist√¢ncia entre duas amostras (compara√ß√£o entre elas).

- *H0: A amostra segue a distribui√ß√£o de refer√™ncia*
- *H1: A amostra n√£o segue a distribui√ß√£o de refer√™ncia*

##### Teste de Anderson-Darling 
> Testa se uma fun√ß√£o de distribui√ß√£o acumulada f(x), pode ser candidata a ser um fun√ß√£o de distribui√ß√£o acumulada de uma amostra aleat√≥ria;

- *H0: A amostra tem distribui√ß√£o de f(x)*
- *H1: A amostra n√£o tem distribui√ß√£o f(x)*

##### Teste de Shapiro Wilk 
> O teste Shapiro Wilk segue a seguinte equa√ß√£o descrita abaixo. Sendo que xi s√£o os valores da amostra ordenados, no qual valores menores que W s√£o evid√™ncias de que os dados s√£o normais.

![Captura de tela 2024-07-04 191812](https://github.com/HenrySchall/Time-Series/assets/96027335/c9789639-2602-44bb-a9f3-491b92b65310)

> J√° o termo b √© determinado pela seguinte equa√ß√£o:

![Captura de tela 2024-07-04 192115](https://github.com/HenrySchall/Time-Series/assets/96027335/c2594f21-082f-4f6d-9293-66b45b0125fb)

> onde ai s√£o constantes geradas pelas m√©dias, vari√¢ncias e covari√¢ncias das estat√≠sticas de ordem de uma amostra de tamanho n de uma distribui√ß√£o normal (tabela da normal).

Estat√≠stica de teste:
- *H0: A amostra segue uma distribui√ß√£o normal (W-obtido < W-cr√≠tico)*
- *H1: A amostra n√£o segue uma distribui√ß√£o normal (W-obtido > W-cr√≠tico)*

![img46](https://github.com/HenrySchall/Time-Series/assets/96027335/64ca5aa1-d601-44b1-8d21-16ec79400211)

##### Teste de Jarque-Bera
> Verifica se os erros s√£o um Ru√≠do Branco, ou seja, seguem uma distribui√ß√£o normal. O teste se baseia nos res√≠duos do m√©todo dos m√≠nimos quadrados. Para sua realiza√ß√£o o teste necessita dos c√°lculos da assimetria (skewness) e da curtose (kurtosis) da amostra, dado pela seguinte f√≥rmula:
 
![Captura de tela 2024-07-04 193133](https://github.com/HenrySchall/Time-Series/assets/96027335/fe76cc80-fa40-46c8-8357-7e19e49339a5)

> onde n e o n√∫mero de observa√ß√µes (ou graus de liberdade geral); S √© aassimetria da amostra; e K √© a curtose da amostra

![Captura de tela 2024-07-04 193243](https://github.com/HenrySchall/Time-Series/assets/96027335/b24d6ca3-6e20-44ed-a3d6-5004c3646bd6)

$\widehat{u3}$ e $\widehat{u4}$ s√£o as estimativas do terceiro e quarto momentos, respectivamente; $\bar{x}$ a m√©dia da amostra, e $ùúé^2$ √© a estimativa do segundo momento, a vari√¢ncia.

- *H0: res√≠duos s√£o normalmente distribu√≠dos*
- *H1: res√≠duos n√£o s√£o normalmente distribu√≠dos*

#### Resumo:

|Teste|Quando usar|Pr√≥s|Contras|Cen√°rios n√£o indicados|
|---|---|---|---|---|
|Shapiro-Wilk|Pequenas amostras (sens√≠vel a pequenas desvios da normalidade)|Sens√≠vel a pequenas desvios da normalidade (adequado para amostras pequenas|Pode ser menos potente em amostras maiores|Dados com distribui√ß√£o fortemente bimodal ou multimodal|
|Kolmogorov-Smirov|Amostras grandes (teste n√£o param√©trico)|N√£o requer suposi√ß√µes sobre os par√¢metros da distribui√ß√£o (adequado para amostras grandes)|Menos sens√≠vel a pequenos desvios (menos potente em amostras pequenas)|Sens√≠vel a desvios nas caudas da distribui√ß√£o|
|Anderson Darling|Verifica√ß√£o geral de normalidade|Sensibilidade a desvios em caudas e simetria (fornece estat√≠stica de teste e valores cr√≠ticos)|Menos sens√≠vel a desvios pequenos|N√£o √© recomendado para amostras muito pequenas|
|Jaque-Bera|Verifica√ß√£o geral de normalidade em amostras grandes|Combina informa√ß√µes sobre simetria e curtose (adequado para amostras grandes)|Menos sens√≠vel a desvios pequenos|Sens√≠vel a desvios nas caudas da distribui√ß√£o|
  
  
##### Teste de Ader√™ncia
> Este teste √© utilizado quando deseja-se validar a hip√≥tese que um conjunto de dados √© gerado por uma determinada distribui√ß√£o de probabilidade.

- *H0: segue o modelo proposto*
- *H1: n√£o segue o modelo proposto*
  
##### Teste de Indeped√™ncia
> Este teste √© utilizado quando deseja-se validar a hip√≥tese de independ√™ncia entre duas vari√°veis aleat√≥rias. Se por exemplo, existe a funl√ßao de probabilidade conjunta das duas vari√°veis aleat√≥rias, pode-se verificar se para todos os poss√≠veis valores das vari√°vies, o produto das probabilidades margianis √© igual √† probabilidade conjunto.

- *H0: as vari√°veis aleat√≥rias s√£o independentes*
- *H1: as vari√°veis aleat√≥rias n√£o s√£o independentes*

##### Teste de Homogeneidade
> Esse teste √© utilizado quando deseja-se validar a hip√≥tese de que uma vari√°vel aleat√≥ria apresenta comportamento similar, ou homog√™neo, em rela√ß√£o √†s suas v√°rias subpopula√ß√µes. Este teste apresenta a mesma mec√¢nica do Teste de Independ√™ncia, mas uma distin√ß√£o importante se refere √† forma como as amostras s√£o coletadas. No Teste de homogeneidade fixa-se o tamanho da amostra em cada uma das subpopula√ß√µes e, ent√£o, seleciona-se uma amostra de cada uma delas.

- *H0: As subpopula√ß√µes das vari√°veis aleat√≥rias s√£o homog√™neas*
- *H1: As subpopula√ß√µes das vari√°veis aleat√≥rias n√£o s√£o homog√™neas*
  

#### Coeficientes de Correla√ß√£o
> O coeficiente de correla√ß√£o verificar a exist√™ncia de associa√ß√£o entre dois conjuntos de dados e tamb√©m o seu grau desta associa√ß√£o.

##### Pearson 
> Estabelecer o n√≠vel da rela√ß√£o linear entre duas vari√°veis. Em outras palavras, mede em grau e sentido (crescente/decrescente) da associa√ß√£o linear entre duas vari√°veis. Sendo definido pela equa√ß√£o:

![1](https://github.com/HenrySchall/Time-Series/assets/96027335/8f4dd7f6-e82b-4bf0-a06d-58400ede1060)

> Deve-se lembrar que o coeficiente de correla√ß√£o populacional √© dado por:

![2](https://github.com/HenrySchall/Time-Series/assets/96027335/6e39a2b7-4bfd-4d30-987e-bca3e8c5c8d8)

![3](https://github.com/HenrySchall/Time-Series/assets/96027335/5391579e-90f0-4ed2-92a0-b95c6068591f)

> O coeficiente de correla√ß√£o de Pearson esta sempre entre ‚àí1,00 e +1,00, onde  o sinal indica a dire√ß√£o, se a correla√ß√£o √© positiva (direta) ou negativa (inversa). O valor do coeficiente de Pearson indica a for√ßa da correla√ß√£o, onde nos  intervalos (+0,90; +1,00) ou (‚àí1,00; ‚àí0,90) indica muito forte correla√ß√£o linear, (+0,60; +0,90) ou (‚àí0,90; ‚àí0,60) indica uma forte correla√ß√£o linear, entre (+0,30; +0,60) ou (‚àí0,60; ‚àí0,30) indica uma moderada correla√ß√£o linear e entre (0,00; +0,30) ou (‚àí0,30; 0,00) indica uma fraca correla√ß√£o linear.
> 
> Exemplo: Uma amostra com 15 observa√ß√µes do tempo de entrega (em minutos) de pizza de uma Pizzaria Delivery  e a dist√¢ncia de entrega. Deseja se verificar se as vari√°veis s√£o correlacionadas. O gr√°fico de dispers√£o das 
vari√°veis, abaixo, sugere que h√° uma rela√ß√£o positiva e linear. Desta forma, utilizar-se-√° o coeficiente de correla√ß√£o de Pearson para checar ser as vari√°veis s√£o correlacionadas. 

|Tempo|Dist√¢ncia|
|---|---|
|40|688|
|21|215|
|14|255|
|20|462|
|24|448|
|29|776|
|15|200|
|19|132|
|10|36|
|35|770|
|18|140|
|52|810|
|19|450|
|20|635|
|11|150|

![4](https://github.com/HenrySchall/Time-Series/assets/96027335/a224016f-5d0a-4b84-ac79-6b8f5dae6ae1)

> Conclui-se que existe uma rela√ß√£o linear forte e positiva entre as vari√°veis. Todavia o coeficiente de correla√ß√£o de Pearson √© apenas uma estimativa do coeficiente de correla√ß√£o populacional, pois √© calculado com base em uma amostra aleat√≥ria de ùëõ pares de dados. Sendo assim a amostra observada pode apresentar correla√ß√£o, mas a popula√ß√£o n√£o, neste caso, tem-se um problema de infer√™ncia, pois r‚â†0 n√£o √© garantia de que ùúå‚â†0. Para resolver esse problema, utiliza-se da estat√≠stica de teste T-student, definido pela equa√ß√£o abaixo, que verificar se realmente existe correla√ß√£o linear entre as vari√°veis:

![5](https://github.com/HenrySchall/Time-Series/assets/96027335/84310f44-b3d8-477d-9e71-1ec81dcbb21a)

> Onde ùë° segue uma distribui√ß√£o ùë°‚àíùëÜùë°ùë¢ùëëùëíùëõùë° com ùëõ‚àí2 graus de liberdade e regido pelas seguintes hip√≥teses:

- *H0: A correla√ß√£o entre as vari√°veis √© zero (ùúå = 0)*
- *H1: A correla√ß√£o entre as vari√°veis n√£o √© zero (ùúå ‚â† 0)*

![6](https://github.com/HenrySchall/Time-Series/assets/96027335/ccc5a428-cea3-4a8f-a773-c6b454b87f28)

> A partir da estat√≠stica ùë° com 13 graus de liberdade, os pontos cr√≠ticos ¬±2,1604. Portanto, rejeita-se ùêªùëú ao n√≠vel de signific√¢ncia de 5%. Ou seja, a correla√ß√£o entre o tempo de entrega e a dist√¢ncia percorrida √© diferente de zero, ent√£o, existe uma rela√ß√£o linear e positiva da ordem de  ùëü = 0,8216;

##### Spearman
> O coeficiente de correla√ß√£o de Spearman, ou rho de Spearman, √© uma medida n√£o param√©trica da correla√ß√£o (associa√ß√£o) entre duas vari√°veis ordinais. Ao contr√°rio do coeficiente de correla√ß√£o de Pearson, que mede a for√ßa e a dire√ß√£o da rela√ß√£o linear entre duas vari√°veis cont√≠nuas, o coeficiente de Spearman avalia intensidade (o qu√£o bem) √© a rela√ß√£o rela√ß√£o entre as duas vari√°veis. O coeficiente de correla√ß√£o de Spearman (ùúå) √© calculado utilizando a seguinte f√≥rmula:

![8](https://github.com/HenrySchall/Time-Series/assets/96027335/3c54cc79-4d8f-4db8-9d87-6eaecd96ec36)

#### Interpreta√ß√£o:
- œÅ=1 indica uma perfeita correla√ß√£o positiva.
- œÅ=‚àí1 indica uma perfeita correla√ß√£o negativa.
- œÅ=0 indica aus√™ncia de correla√ß√£o.
 
> Utilizando-se da mesma equa√ß√£o estat√≠stica de teste T-student do coeficiente de correla√ß√£o de Pearson. Teremos as seguintes hip√≥teses:

- *H0: A correla√ß√£o entre as vari√°veis √© zero*
- *H1: A correla√ß√£o entre as vari√°veis n√£o √© zero*

> Exemplo: dados os valores da tabela abaixo:

![9](https://github.com/HenrySchall/Time-Series/assets/96027335/68db17b8-2b31-41c9-b1d1-cf241b30ee55)

![12](https://github.com/HenrySchall/Time-Series/assets/96027335/1c332788-5570-48e1-8d17-2b3b36c61aff)

> Dado que ùë° ~ ùë°ùëõ‚àí2;ùõº2 ‚ÅÑ , tem-se, a partir da estat√≠stica ùë°‚àíùëÜùë°ùë¢ùëëùëíùëõùë° com 11 graus 
de liberdade, os pontos cr√≠ticos ¬±2,2010. Portanto, rejeita-se ùêªùëú ao n√≠vel de 
signific√¢ncia de 5%. Ou seja, a correla√ß√£o entre as vari√°veis ùëã e ùëå √© diferente 
de zero, ent√£o, existe uma rela√ß√£o n√£o-linear e negativa da ordem de ùëü=
 ‚àí0,9698. 

##### Kendall 
> O coeficiente de correla√ß√£o de Kendall √© uma medida estat√≠stica utilizada para avaliar a associa√ß√£o entre duas vari√°veis ordinaiss, como no caso do coeficiente de correla√ß√£o de Spearman. Ele √© particularmente √∫til quando as vari√°veis em quest√£o n√£o assumem necessariamente distribui√ß√µes normais. O coeficiente de correla√ß√£o de Kendall (œÑ) √© calculado utilizando as seguintes f√≥rmulas:

![7](https://github.com/HenrySchall/Time-Series/assets/96027335/a9c81241-d071-40d6-aa6e-2c58ab8ca5ba)

#### Interpreta√ß√£o:
- œÑ=1 indica uma perfeita concord√¢ncia.
- œÑ=‚àí1 indica uma perfeita discord√¢ncia.
- œÑ=0 indica aus√™ncia de associa√ß√£o entre as vari√°veis.







